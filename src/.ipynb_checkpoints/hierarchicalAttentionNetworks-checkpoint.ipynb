{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention Network\n",
    "# https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael Guarino\n",
    "\n",
    "MSCS688 Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Classification tasks are a central part of understanding language and are a core task of Natural Language Processing. For a machine learning algorithm to perform an classification task on a body of text it must learn to correctly assign labels to the text. These labels could correspond to sentiment, risk, or the presence of certain features deamed to be important. Traditional approaches to text classification represent document with spare lexical features and use linear learners to perform the classification task. Recently as deep learning has been shown to achieve state of the art performance in many natural langauge processing tasks specifically in classification tasks. These models represent the whole text in high dimensional space and use extremely expressive deep learning models as their learners. This work is the ensemble of several different works over the course of the semester on different methods of performing document classification using, several CNN based approaches, dynamic LSTM, standard LSTM, and the Hierarchical Attention Network as the capstone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed System Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Hierarchical Attention Network is a novel deep learning architecture that takes advantage of the hierchical structure of documents to constructure a more detailed representation of the document. As words form sentences and sentences form the document, the Hierarchical Attention Network's representation of the document uses this hierarchy in order to determine what sentences and what words in those sentences are most important in the classification of the document as a whole. \n",
    "    This model uses two levels of GRU word and sentences level encoders in in order to build the word and sentence level representations of the document. It then uses the attention mechanism in order to allow the network to attribute importance to different sentences and words in those sentences based on context. There are two attention mechanisms that attend over of the word level encoder and the sentence level encoder. These allow the model to construct a representation of the document that attribute greater levels of importance to key sentences and words throughout the document. The importance of the application of the attention mechanism is this architecture that separates the Hierarchical attention mechanism from other works of it's kind it that the attention mechanisms which preside over the their respective sequences use context to learn when a sequence's tokens are of importance to the classification of the document.\n",
    "    As part of a preprocessing step before lanaguage is fed through the Hierarchical Attention Network a vector representation of the document is formed using a word embedding. The value of the word embedding is that words that appear in similar context appear closer together in vector space. The importance of this is based on the Distributional Hypothesis that word that appear in the same context share semantic meaning.\n",
    "    Embedded representations of the document first begin being processed by the Hierarchical Attention Network at the word level using a Bidirectional Recurrent Neural Network composed of Gated Recurrent Unit (GRU) cells. The word level encoder composed of GRU purposed by 'Hierarchical Attention Networks for Document Classification' encodes every sentence in the document as a series of words (1). The sentence level encoder composed also of Gated Recurrent Unit (GRU) cells is then used to encode every document as a series of sentences (1).\n",
    "    The Gated Recurrent Unit has become a popular alternative to the Long Short Term Memory cell (LSTM) because the GRU has fewer learned parameters than the LSTM but still has a similar powerful gating mechanism that allows it to be resilient against the vanishing/exploding gradient problem that Vanilla Recurrent Neural Networks are known for. The GRU has a reset gate and an update gate that allow past information throughout the sequence to be retained in the memory of the GRU cell; however, becuase the GRU has fewer learned parameters than the LSTM it has been said to be less expressive in it's ability to capture sequential information (2). \n",
    "    The gating mechism of the LSTM is very robust as it has many learned parameters. These learned parameters of the gating mechanism allow the LSTM to protect against the vanishing/exploding gradient problem quiet effectively and make it extremely expressive at capturing sequential information contained within the sequence; however, that does come at the cost of increased computational complexity. As part of this work we use the LSTM for the word and sentence level encoders in order to compare results with those of the 'Hierarchical Attention Networks for Document Classification' (1).\n",
    "    The implementation of the word and sentence level attention mechanism allows for the representation of different importances throughout document. Attention is implemented by allowing input from the sequence to be passed along to the attending sequence encoder. This is done through the creation of an attention distribution in which different time step's hidden states are learned to be given greater importance to the importance of the overall task, which in this case is the classification of the document. Through the application of several different levels of the attention mechanism the Hierarchical Attention Network learns what sentences and words are important in the successful completion of it's task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Tensorflow 1.1\n",
    " - Keras version 20\n",
    " - all experiments were run on a machine with intel i7 cpu and Nvidia 1080TI gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Literature Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    As part of our investigation of document classification using deep learning we performed serveral experiments using CNNs, LSTMs, Dynamic Length LSTM, and Bidirectional LSTMs as was sited in this literature. We were most interested in the use of the attention mechanism as originall site in 'Sequence to Sequence Learning with Neural Networks' (3) as a novel approach to allow different encoders to exchange information.\n",
    "    There has been many different approaches taken "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A experiments were performed on the Stanford IMDB dataset which is a natural langauge dataset where movie review have labels that describe the sentiment described in the movie review. There are 8 different classes that describe the sentiment from 0-3 for negative sentiment to 6-10 for positive sentiment. Here are the accuricies achieved by the different networks.\n",
    "    - Hierarchical Attention Network as described in this paper (1): NA\n",
    "    - CNN as described in Yoon (4): 0.885(word level), 0.877(character level)\n",
    "    - Standard LSTM: (word level) 0.834\n",
    "    - Bidirectional LSTM: (word level) 0.837\n",
    "    - Dynamic LSTM: (word level) 0.834\n",
    "        respectively found in...\n",
    "        -- hierarchicalAttentionNetworks.ipynb\n",
    "        -- cnn.py\n",
    "        -- nn-imdb-charLevel-text-classification.py\n",
    "        -- lstm-imdb-wordLevel-text-classification.py\n",
    "        -- BidirectionalLSTM-imdb-charLevel-text-classification.ipynb\n",
    "        -- BidirectionalLSTM-imdb-wordLevel-text-classification.ipynb\n",
    "        -- dynamic_lstm-imdb-wordLevel-text-classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Zichao, Yang\n",
    "    Hierarchical Attention Networks for Document Classification\n",
    "    https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\n",
    "2) Jozefowicz, Rafal. An Empirical Exploration of Recurrent Network Architectures\n",
    "    http://proceedings.mlr.press/v37/jozefowicz15.pdf\n",
    "3) Sutskever, Llya. Sequence to Sequence Learning with Neural Networks. \n",
    "    https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "    Accessed 25 Aug. 2017.\n",
    "4) Kim, Yoon. Convolutional Neural Networks for Sentence Classification. \n",
    "    http://www.aclweb.org/anthology/D14-1181\n",
    "    Accessed 25 Aug. 2017.\n",
    "5) Zhou, Peng. Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification \n",
    "    http://aclweb.org/anthology/P/P16/P16-2034.pdf\n",
    "    Accessed 25 Aug. 2017.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "print('current version of tf:{}'.format(tf.__version__))\n",
    "\n",
    "from utils import prjPaths\n",
    "\n",
    "class HierarchicalAttentionNetwork:\n",
    "    def __init__(self, vocab_size, num_classes, max_seq_len):\n",
    "\n",
    "        ## Parameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 128\n",
    "        self.display_step = 10\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.embedding_dim = 128\n",
    "        self.word_encoder_num_hidden=max_seq_len\n",
    "        self.word_output_size=max_seq_len,\n",
    "        self.sentence_output_size=max_sentence_len=50,\n",
    "        self.sentence_encoder_num_hidden=max_sentence_len,\n",
    "        self.max_grad_norm=5.0,\n",
    "        self.dropout_keep_proba=0.5,\n",
    "        \n",
    "        # tf graph input\n",
    "        self.inputs = tf.placeholder(shape=[None, None, None], dtype=tf.int32, name='inputs') # [document, sentence, word]\n",
    "        self.word_lengths = tf.placeholder(shape=[None, None], dtype=tf.int32, name='word_lengths') # [document, sentence]\n",
    "        self.sentence_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='sentence_lengths') # [document]\n",
    "        self.y = tf.placeholder(shape=(None,), dtype=tf.int32, name='y') # [document level]\n",
    "        self.sample_weights = tf.placeholder(shape=(None,), dtype=tf.float32, name='sample_weights')\n",
    "\n",
    "        (self.document_size, self.sentence_size, self.word_size) = tf.unstack(tf.shape(self.inputs))\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        with tf.variable_scope('training') as scope:\n",
    "\n",
    "            self.embedding_layer()\n",
    "            self.run() # word_level encoder with word attention and sentence_level encoder with sentence attention connected to fully connected layer making prediction\n",
    "\n",
    "            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.multiply(self.cross_entropy, self.sample_weights))\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(self.logits, self.y, 1), tf.float32))\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "            train_vars = tf.trainable_variables()\n",
    "            grads, global_norm = tf.clip_by_global_norm(tf.gradients(self.loss, train_vars), self.max_grad_norm)\n",
    "            tf.summary.scalar('global_grad_norm', self.global_norm)\n",
    "\n",
    "            opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.train_op = opt.apply_gradients(zip(grads, train_vars), name='train_op', global_step=global_step)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    ## Embedding Layer\n",
    "    def embedding_layer(self):\n",
    "        # create embedding layer fyi no gpu support for tf.nn.embedding_lookup\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding_layer\"):\n",
    "            self.w = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_dim], -1.0, 1.0), dtype=tf.float32,  name=\"w\") # embedding matrix\n",
    "            self.inputs_embedded = tf.nn.embedding_lookup(w, self.inputs) # [None, max_seq_len, embedding_size]\n",
    "\n",
    "    ## Birdirectional RNN Layer\n",
    "    def bidirectional_RNN(self, num_hidden, inputs, input_lengths, scope=None):\n",
    "        encoder_fw_cell = rnn.LSTMCell(num_hidden)\n",
    "        encoder_bw_cell = rnn.LSTMCell(num_hidden)\n",
    "        with tf.variable_scope(scope or \"bidirectional_RNN\") as scope:\n",
    "\n",
    "            ((encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_final_state, encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_fw_cell,\n",
    "                                                                                                                                           cell_bw=encoder_bw_cell,\n",
    "                                                                                                                                           inputs=inputs,\n",
    "                                                                                                                                           dtype=tf.float32,\n",
    "                                                                                                                                           scope=scope,\n",
    "                                                                                                                                           time_major=True)\n",
    "            def concatenate_state(self, fw_state, bw_state):\n",
    "                state_c = tf.concat((fw_state.c, bw_state.c), 1, name='bidirectional_concat_c')\n",
    "                state_h = tf.concat((fw_state.h, bw_state.h), 1, name='bidirectional_concat_h')\n",
    "                state = rnn.LSTMStateTuple(c=state_c, h=state_h)\n",
    "                return state\n",
    "            # end\n",
    "\n",
    "            # Concatenates forward and backwards encoder states along axis 2\n",
    "            encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "            encoder_state = concatenate_state(encoder_fw_final_state, encoder_bw_final_state)\n",
    "\n",
    "            return encoder_outputs, encoder_state\n",
    "    # end\n",
    "\n",
    "    ## Attention\n",
    "    def word_attention(self, inputs, output_size, scope=None):\n",
    "        with tf.variable_scope(scope or 'attention') as scope:\n",
    "            attention_context_vector_uw = tf.get_variable(name='attention_context_vector',\n",
    "                                                           shape=[output_size],\n",
    "                                                           initializer=layers.xavier_initializer(),\n",
    "                                                           dtype=tf.float32)\n",
    "            input_projection_u = layers.fully_connected(inputs, output_size,\n",
    "                                                           activation_fn=tf.tanh,\n",
    "                                                           scope=scope)\n",
    "            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n",
    "            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n",
    "            weighted_projection = tf.multiply(input_projection, attention_weights)\n",
    "            outputs = tf.reduce_sum(weighted_projection, axis=1)\n",
    "            return outputs\n",
    "    # end\n",
    "\n",
    "    def run(self):\n",
    "        self.word_level_inputs = tf.reshape(self.inputs_embedded, [self.document_size * self.sentence_size, self.word_size, self.embedding_dim])\n",
    "        self.word_level_lengths = tf.reshape(self.word_lengths, [self.document_size * self.sentence_size])\n",
    "\n",
    "        with tf.variable_scope('word_level'):\n",
    "            word_encoder_outputs, word_encoder_state = self.bidirectional_RNN(num_hidden=word_encoder_num_hidden, \n",
    "                                                                           inputs=word_encoder_inputs_embedded, \n",
    "                                                                           input_lengths=self.word_level_lengths, \n",
    "                                                                           scope=scope)\n",
    "            word_level_output = self.word_attention(inputs=word_encoder_outputs, output_size=self.word_output_size)\n",
    "            word_level_output = layers.dropout(word_level_output, keep_prob=self.dropout_keep_proba)\n",
    "\n",
    "\n",
    "        sentence_encoder_inputs = tf.reshape(word_level_output, [self.document_size, self.sentence_size, self.word_output_size])\n",
    "\n",
    "        with tf.variable_scope('sentence_level'):\n",
    "            sentence_encoder_outputs, sentence_encoder_state = bidirectional_RNN(num_hidden=sentence_encoder_num_hidden, \n",
    "                                                                                   inputs=sentence_encoder_inputs, \n",
    "                                                                                   input_lengths=self.sentence_lengths,\n",
    "                                                                                   scope=scope)\n",
    "            sentence_level_output = word_attention(inputs=sentence_encoder_outputs, output_size=self.sentence_output_size)\n",
    "            sentence_level_output = layers.dropout(sentence_level_output, keep_prob=self.dropout_keep_proba)\n",
    "\n",
    "        with tf.variable_scope('classifier'):\n",
    "            self.logits = layers.fully_connected(sentence_level_output, self.y, activation_fn=None)\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "\n",
    "print('Loading data...\\n')\n",
    "if not IMDB.csvExist():\n",
    "    imdb = IMDB(action='create')\n",
    "    imdb.createManager()\n",
    "    x_train, y_train, x_test, y_test = imdb.partitionManager()\n",
    "else:\n",
    "    imdb = IMDB()\n",
    "    x_train, y_train, x_test, y_test = imdb.partitionManager()\n",
    "    \n",
    "# Generate batches for one epoch\n",
    "#batches = imdb.get_batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "model = HierarchicalAttentionNetwork(imdb.vocab_size, num_classes=len(y_test[0]), max_seq_len=imdb.max_seq_len)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for batch in list(zip(x_train, y_train)):\n",
    "        feed_dict = {inputs:batch[0],\n",
    "                     y:batch[1],\n",
    "                     word_lengths:[1, [len(i) for i in batch[0]]],\n",
    "                     sentence_lengths:len(batch[0])\n",
    "                    }\n",
    "        sess.run(model.train().logits, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...\\n')\n",
    "from dataProcessing import IMDB\n",
    "\n",
    "if not IMDB.csvExist():\n",
    "    imdb = IMDB(action='create')\n",
    "    imdb.createManager()\n",
    "    x_train, y_train, x_test, y_test = imdb.partitionManager()\n",
    "else:\n",
    "    imdb = IMDB()\n",
    "    x_train, y_train, x_test, y_test = imdb.partitionManager()\n",
    "#print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
